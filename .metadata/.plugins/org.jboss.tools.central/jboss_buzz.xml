<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Simulating CloudEvents with AsyncAPI and Microcks</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CKaov6paM0c/simulating-cloudevents-asyncapi-and-microcks" /><author><name>Laurent Broudoux</name></author><id>6b56bdb0-3134-4971-9897-3d172f4ea6ad</id><updated>2021-06-02T07:00:00Z</updated><published>2021-06-02T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/event-driven"&gt;Event-driven architecture&lt;/a&gt; was an evolutionary step toward cloud-native applications, and supports &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; applications. Events connect &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;, letting you decouple functions in space and time and make your applications more resilient and elastic.&lt;/p&gt; &lt;p&gt;But events come with challenges. One of the first challenges for a development team is how to describe events in a repeatable, structured form. Another challenge is how to work on applications that consume events without having to wait for another team to hand you the applications that produce those events.&lt;/p&gt; &lt;p&gt;This article explores those two challenges and shows how to simulate events using &lt;a href="https://cloudevents.io"&gt;CloudEvents&lt;/a&gt;, &lt;a href="https://asyncapi.com"&gt;AsyncAPI&lt;/a&gt;, and &lt;a href="https://microcks.io"&gt;Microcks&lt;/a&gt;. CloudEvents and AsyncAPI are complementary specifications that you can combine to help define an event-driven architecture. Microcks allows simulation of CloudEvents to speed up and protect the autonomy of development teams.&lt;/p&gt; &lt;h2&gt;CloudEvents or AsyncAPI?&lt;/h2&gt; &lt;p&gt;New standards such as &lt;a href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt; or &lt;a href="https://www.asyncapi.com/"&gt;AsyncAPI&lt;/a&gt; have emerged to address the need to describe events in a structured format. People often ask: "Should I use CloudEvents or AsyncAPI?" There's a widespread belief that CloudEvents and AsyncAPI compete within the same scope. I see things differently, and in this article, I'll explain how the two standards work well together.&lt;/p&gt; &lt;h3&gt;What is CloudEvents?&lt;/h3&gt; &lt;p&gt;The essence of CloudEvents can be found on a statement from &lt;a href="http://cloudevents.io/"&gt;its website&lt;/a&gt;:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;em&gt;CloudEvents is a specification for describing event data in common formats to provide interoperability across services, platforms, and systems.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The purpose of CloudEvents is to establish a common format for event data description. CloudEvents is part of the Cloud Native Computing Foundation's &lt;a href="https://github.com/cncf/wg-serverless"&gt; Serverless Working Group&lt;/a&gt;. A lot of integrations already exist within &lt;a href="https://knative.dev/docs/eventing/"&gt;Knative Eventing&lt;/a&gt; (or &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;Red Hat OpenShift Serverless&lt;/a&gt;), &lt;a href="https://www.triggermesh.com/"&gt;Trigger Mesh&lt;/a&gt;, and &lt;a href="https://azure.microsoft.com/en-us/services/event-grid"&gt;Azure Event Grid&lt;/a&gt;, allowing true cross-vendor platform interoperability.&lt;/p&gt; &lt;p&gt;The CloudEvents specification is focused on events and defines a common envelope (set of attributes) for your application event. As of today, CloudEvents proposes two different &lt;a href="https://github.com/cloudevents/spec/blob/v1.0.1/kafka-protocol-binding.md#13-content-modes"&gt;content modes&lt;/a&gt; for transferring events: structured and binary.&lt;/p&gt; &lt;p&gt;The CloudEvents repository offers an &lt;a href="https://gist.github.com/e3261b13eb7a9dbb14ccf59b1580d5b7#file-cloudevent-json"&gt;example of a JSON structure containing event attributes&lt;/a&gt;. This is a &lt;em&gt;structured CloudEvent&lt;/em&gt;. The event data in the example is XML, contained in the value &lt;code&gt;&lt;much wow=\"xml\"/&gt;&lt;/code&gt;, but it can be of any type. CloudEvents takes care of defining meta-information about your event, but does not help you define the actual event content:&lt;/p&gt; &lt;pre&gt; { "specversion" : "1.0.1", "type" : "com.github.pull.create", "source" : "https://github.com/cloudevents/spec/pull/123", "id" : "A234-1234-1234", "time" : "2020-04-05T17:31:00Z", "comexampleextension1" : "value", "comexampleextension2" : { "othervalue": 5 }, "contenttype" : "text/xml", "data" : "&lt;much wow=\"xml\"/&gt;" } &lt;/pre&gt; &lt;h3&gt;What is AsyncAPI?&lt;/h3&gt; &lt;p&gt;To understand AsyncAPI, again we turn to &lt;a href="http://asyncapi.com/"&gt;its website&lt;/a&gt;:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;em&gt;AsyncAPI is an industry standard for defining asynchronous APIs. Our long-term goal is to make working with EDAs as easy as it is to work with REST APIs.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The "API" stands for &lt;a href="https://developers.redhat.com/topics/api-management"&gt;application programming interface&lt;/a&gt; and embodies an application's interactions and capabilities. AsyncAPI can be seen as the sister specification of &lt;a href="https://www.openapis.org/"&gt;OpenAPI&lt;/a&gt;, but targeting asynchronous protocols based on event brokering.&lt;/p&gt; &lt;p&gt;AsyncAPI focuses on the application and the communication channels it uses. Unlike CloudEvents, AsyncAPI does not define how your events should be structured. However, AsyncAPI provides an extended means to precisely define both the meta-information and the actual content of an event.&lt;/p&gt; &lt;p&gt;An &lt;a href="https://gist.github.com/67252933bcfea50c996b44dd20225962#file-asyncapi-yml"&gt;example in YAML&lt;/a&gt; can be found on GitHub. This example describes an event with the title &lt;code&gt;User signed-up event&lt;/code&gt;, published to the &lt;code&gt;user/signedup&lt;/code&gt; channel. These events have three properties: &lt;code&gt;fullName&lt;/code&gt;, &lt;code&gt;email&lt;/code&gt;, and &lt;code&gt;age&lt;/code&gt; . Each property is defined using semantics from &lt;a href="https://json-schema.org/"&gt;JSON Schema&lt;/a&gt;. Although it's not shown in this example, AsyncAPI also allows you to specify event headers and whether these events will be available through different protocol bindings such as &lt;a href="https://kafka.apache.org/"&gt;Kafka&lt;/a&gt;, &lt;a href="https://www.amqp.org/"&gt;AMQP&lt;/a&gt;, &lt;a href="https://mqtt.org/"&gt;MQTT&lt;/a&gt;, or &lt;a href="https://www.w3.org/TR/websockets/"&gt;WebSocket&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;asyncapi: 2.0.0 id: urn:com.asyncapi.examples.user info: title: User signed-up event version: 0.1.1 channels: user/signedup: publish: message: payload: type: object properties: fullName: type: string email: type: string format: email age: type: integer minimum: 18&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;CloudEvents with AsyncAPI&lt;/h2&gt; &lt;p&gt;The explanations and examples I've shown reveal that the CloudEvents with AsyncAPI standards tackle different scopes. Thus they do not have to be treated as mutually exclusive. You can actually combine them to achieve a complete event specification, including application definition, channels description, structured envelope, and detailed functional data carried by the event.&lt;/p&gt; &lt;p&gt;The general idea behind the combination is to use an AsyncAPI specification as a hosting document. It holds references to CloudEvents attributes and adds more details about the event format.&lt;/p&gt; &lt;p&gt;You can use two mechanisms in AsyncAPI to ensure this combination. Choosing the correct mechanism might depend on the protocol you choose to convey your events. Things aren't perfect yet and you'll have to make a choice.&lt;/p&gt; &lt;p&gt;Let's take the example of using &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; to distribute events:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;In the structured content mode, CloudEvents meta-information is tangled up with the &lt;code&gt;data&lt;/code&gt; in the messages value. For that mode, we'll use the JSON Schema composition mechanism that is accessible from AsyncAPI.&lt;/li&gt; &lt;li&gt;In the binary content mode (for which we can use &lt;a href="https://avro.apache.org/"&gt;Avro&lt;/a&gt;), meta-information for each event is dissociated from the message value and inserted, instead, into the header of each message. For that, we'll use the &lt;a href="https://www.asyncapi.com/docs/specifications/2.0.0#messageTraitObject"&gt;MessageTrait&lt;/a&gt; application mechanism present in AsyncAPI.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Structured content mode&lt;/h3&gt; &lt;p&gt;This section rewrites the previous AsyncAPI example to use CloudEvents in structured content mode. The resulting &lt;a href="https://gist.github.com/035ccc4d7b7cdd414f0ebc5a53e80c4c#file-asyncapi-ce-yaml"&gt;definition&lt;/a&gt; contains the following elements worth noting:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The definition of &lt;code&gt;headers&lt;/code&gt; that starts on line 16 contains our application's &lt;code&gt;custom-header&lt;/code&gt;, as well as the mandatory CloudEvents &lt;code&gt;content-type&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;schemas&lt;/code&gt; field refers to the CloudEvents specification on line 33, re-using this specification as a basis for our message.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;schemas&lt;/code&gt; field also refers to a refined version of the &lt;code&gt;data&lt;/code&gt; property description on line 36.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Here is the definition:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;asyncapi: '2.0.0' id: 'urn:io.microcks.example.user-signedup' info: title: User signed-up CloudEvents API structured version: 0.1.3 defaultContentType: application/json channels: user/signedup: subscribe: message: bindings: kafka: key: type: string description: Timestamp of event as milliseconds since 1st Jan 1970 headers: type: object properties: custom-header: type: string content-type: type: string enum: - 'application/cloudevents+json; charset=UTF-8' payload: $ref: '#/components/schemas/userSignedUpPayload' examples: [...] components: schemas: userSignedUpPayload: type: object allOf: - $ref: 'https://raw.githubusercontent.com/cloudevents/spec/v1.0.1/spec.json' properties: data: $ref: '#/components/schemas/userSignedUpData' userSignedUpData: type: object properties: fullName: type: string email: type: string format: email age: type: integer minimum: 18&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Binary content mode&lt;/h3&gt; &lt;p&gt;Now, we'll apply the binary content mode to the AsyncAPI format. The resulting &lt;a href="https://gist.github.com/d5eca1c76fd57e5b3326b5d5db26bbd3#file-asyncapi-ce-yaml"&gt;definition&lt;/a&gt; shows that event properties have moved out of this format. Other important things to notice here are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A trait is applied at the message level on line 16. The trait resource is a partial AsyncAPI document containing a &lt;code&gt;MessageTrait&lt;/code&gt; definition. This trait will bring in all the mandatory attributes (&lt;code&gt;ce_*&lt;/code&gt;) from CloudEvents. It is the equivalent of the CloudEvents JSON Schema.&lt;/li&gt; &lt;li&gt;This time we're specifying our event payload using an Avro schema as specified on line 25.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Here is the definition:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;asyncapi: '2.0.0' id: 'urn:io.microcks.example.user-signedup' info: title: User signed-up CloudEvents API binary version: 0.1.3 channels: user/signedup: subscribe: message: bindings: kafka: key: type: string description: Timestamp of event as milliseconds since 1st Jan 1970 traits: - $ref: 'https://raw.githubusercontent.com/microcks/microcks-quickstarters/main/cloud/cloudevents/cloudevents-v1.0.1-asyncapi-trait.yml' headers: type: object properties: custom-header: type: string contentType: avro/binary schemaFormat: application/vnd.apache.avro+json;version=1.9.0 payload: $ref: './user-signedup.avsc#/User' examples: [...] &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;What are the benefits of combining CloudEvents with AsyncAPI?&lt;/h3&gt; &lt;p&gt;Whichever content mode you chose, you now have a comprehensive description of your event and all the elements of your event-driven architecture. The description guarantees the low-level interoperability of the CloudEvents-plus-AsyncAPI combination, along with the ability to be routed and trigger a function in a serverless world. In addition, you provide a complete description of the carried &lt;code&gt;data&lt;/code&gt; that will be of great help for applications consuming and processing events.&lt;/p&gt; &lt;h2&gt;Simulating CloudEvents with Microcks&lt;/h2&gt; &lt;p&gt;Let's tackle the second challenge stated at the beginning of this article: How can developers efficiently work as a team without having to wait for someone else's events? We've seen how to fully describe events. However, it would be even better to have a pragmatic approach for leveraging this CloudEvents-plus-AsyncAPI contract. That's where &lt;a href="https://microcks.io/"&gt;Microcks&lt;/a&gt; comes to the rescue.&lt;/p&gt; &lt;h3&gt;What is Microcks?&lt;/h3&gt; &lt;p&gt;Microcks is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;-native tool for mocking/simulating and testing APIs. One purpose of Microcks is to turn your API contract (such as OpenAPI, AsyncAPI, or the &lt;a href="https://getpostman.com/"&gt;Postman&lt;/a&gt; collection) into live mocks in seconds. Once it has imported your AsyncAPI contract, Microcks starts producing mock events on a message broker at a defined frequency.&lt;/p&gt; &lt;p&gt;Using Microcks you can simulate CloudEvents in seconds, without writing a single line of code. Microcks allows the team that is relying on input events to immediately start working. They do not have to wait for the team that is coding the application that will publicize events.&lt;/p&gt; &lt;h3&gt;Using Microcks for CloudEvents&lt;/h3&gt; &lt;p&gt;To produce CloudEvents events with Microcks, simply re-use examples by adding them to your contract. We omitted the &lt;code&gt;examples&lt;/code&gt; property before, but we'll now add that property to our &lt;a href="https://gist.github.com/820c925b8ff84929ebf0c30ad1900c62#file-asyncapi-ce-yaml"&gt;example in binary content mode&lt;/a&gt; on line 26:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;asyncapi: '2.0.0' id: 'urn:io.microcks.example.user-signedup' info: title: User signed-up CloudEvents API binary version: 0.1.3 channels: user/signedup: subscribe: message: bindings: kafka: key: type: string description: Timestamp of event as milliseconds since 1st Jan 1970 traits: - $ref: 'https://raw.githubusercontent.com/microcks/microcks-quickstarters/main/cloud/cloudevents/cloudevents-v1.0.1-asyncapi-trait.yml' headers: type: object properties: custom-header: type: string contentType: avro/binary schemaFormat: application/vnd.apache.avro+json;version=1.9.0 payload: $ref: './user-signedup.avsc#/User' examples: - john: summary: Example for John Doe user headers: ce_specversion: "1.0" ce_type: "io.microcks.example.user-signedup" ce_source: "/mycontext/subcontext" ce_id: "{{uuid()}}" ce_time: "{{now(yyyy-MM-dd'T'HH:mm:SS'Z')}}" content-type: application/avro sentAt: "2020-03-11T08:03:38Z" payload: fullName: John Doe email: john@microcks.io age: 36&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Key points to note are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;You can put in as many examples as you want because this property becomes a map in AsyncAPI.&lt;/li&gt; &lt;li&gt;You can specify both &lt;code&gt;headers&lt;/code&gt; and &lt;code&gt;payload&lt;/code&gt; values.&lt;/li&gt; &lt;li&gt;Even if &lt;code&gt;payload&lt;/code&gt; will be Avro-binary encoded, you use YAML or JSON to specify examples.&lt;/li&gt; &lt;li&gt;You can use templating functions through the &lt;code&gt;{{ }}&lt;/code&gt; notation to introduce &lt;a href="https://microcks.io/documentation/using/advanced/templates/#function-expressions"&gt;random or dynamic values&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Once the schema is imported into Microcks, it discovers the API definition as well as the different examples. Microcks starts immediately producing mock events on the Kafka broker it is connected to—every three seconds in our example (see Figure 1).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-simulating-cloud-events_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-simulating-cloud-events_0.png?itok=uI0qn302" width="600" height="327" alt="A Microcks import of AsyncAPI with CloudEvents." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A Microcks import of AsyncAPI with CloudEvents. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Since release &lt;a href="https://microcks.io/blog/microcks-1.2.0-release/"&gt;1.2.0&lt;/a&gt;, Microcks also supports the connection to a schema registry. Therefore, it publishes the Avro schema used at the mock-message publication time. You can use the &lt;a href="https://github.com/edenhill/kafkacat"&gt;kafkacat&lt;/a&gt; command-line interface (CLI) tool to connect to the Kafka broker and registry, and then inspect the content of mock events. Here we're using the &lt;a href="https://www.apicur.io/registry/"&gt;Apicurio service registry&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ kafkacat -b my-cluster-kafka-bootstrap.apps.try.microcks.io:9092 -t UsersignedupCloudEventsAPI_0.1.3_user-signedup -s value=avro -r http://apicurio-registry.apps.try.microcks.io/api/ccompat -o end -f 'Headers: %h - Value: %s\n' --- OUTPUT % Auto-selecting Consumer mode (use -P or -C to override) % Reached end of topic UsersignedupCloudEventsAPI_0.1.3_user-signedup [0] at offset 276 Headers: sentAt=2020-03-11T08:03:38Z,content-type=application/avro,ce_id=7a8cc388-5bfb-42f7-8361-0efb4ce75c20,ce_type=io.microcks.example.user-signedup,ce_specversion=1.0,ce_time=2021-03-09T15:17:762Z,ce_source=/mycontext/subcontext - Value: {"fullName": "John Doe", "email": "john@microcks.io", "age": 36} % Reached end of topic UsersignedupCloudEventsAPI_0.1.3_user-signedup [0] at offset 277 Headers: ce_id=dde8aa04-2591-4144-aa5b-f0608612b8c5,sentAt=2020-03-11T08:03:38Z,content-type=application/avro,ce_time=2021-03-09T15:17:733Z,ce_type=io.microcks.example.user-signedup,ce_specversion=1.0,ce_source=/mycontext/subcontext - Value: {"fullName": "John Doe", "email": "john@microcks.io", "age": 36} % Reached end of topic UsersignedupCloudEventsAPI_0.1.3_user-signedup [0] at offset 279&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can check that the emitted events respect both the CloudEvents meta-information structure and the AsyncAPI &lt;code&gt;data&lt;/code&gt; definition. Moreover, each event has different random attributes, which allows it to simulate diversity and variation for the consuming application.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has shown how to solve some of the challenges that come with an event-driven architecture.&lt;/p&gt; &lt;p&gt;First, I described how recent standards, CloudEvents and AsyncAPI, focus on different scopes: the event for CloudEvents and the application for AsyncAPI.&lt;/p&gt; &lt;p&gt;Then I demonstrated how to combine the specifications to provide a comprehensive description of all the elements involved in an event-driven architecture: application definition, channels description, structured envelope, and detailed functional data carried by the event. The specifications are complementary, so you can use one or both depending on how deep you want to go in your formal description.&lt;/p&gt; &lt;p&gt;Finally, you've seen how to use Microcks to simulate any events based on AsyncAPI, including those generated by CloudEvents, just by using examples. Microcks answers the challenge of working, testing, and validating autonomously when different development teams are using an event-driven architecture.&lt;/p&gt; &lt;p&gt;I hope you learned something new—if so, please consider reacting, commenting, or sharing. Thanks for reading.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/02/simulating-cloudevents-asyncapi-and-microcks" title="Simulating CloudEvents with AsyncAPI and Microcks"&gt;Simulating CloudEvents with AsyncAPI and Microcks&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CKaov6paM0c" height="1" width="1" alt=""/&gt;</summary><dc:creator>Laurent Broudoux</dc:creator><dc:date>2021-06-02T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/02/simulating-cloudevents-asyncapi-and-microcks</feedburner:origLink></entry><entry><title type="html">Custom Layered Immutable Spring Boot Kie Server</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hYH2jJwUYUQ/custom-layered-immutable-spring-boot-kie-server.html" /><author><name>Gonzalo Muñoz Fernández</name></author><id>https://blog.kie.org/2021/06/custom-layered-immutable-spring-boot-kie-server.html</id><updated>2021-06-01T23:06:08Z</updated><content type="html">“Uncommon thinkers reuse what common thinkers refuse” (J.R.D. Tata) When creating an image for an immutable Spring Boot Kie Server application, it is possible to split up the files and folders belonging to the fat-jar into different layers. Main advantages of this approach are: 1. Libraries, code, and resources are grouped into layers based on the likelihood to change between builds. This reduces the image generation time. 2. Layers downloaded once (saving disk space and bandwidth) and reused for other images. 3. Execution over the unzipped classes is a little bit faster than launching the fat-jar: java -jar app.jar Considering all these points, it is completely meaningful to layer the immutable Spring Boot Kie Server -isolating the KJARs into a new custom layer- as business assets in KJARs are more likely to change. PACKAGING SPRING BOOT KIE SERVER WITH LAYERS The spring-boot-maven-plugin is in charge of creating the immutable fat-jar containing all the KJAR files and their dependencies. For triggering this process, just add the following properties to the Spring Boot application.properties file: kieserver.classPathContainer=true kieserver.autoScanDeployments=true Next, we have to enable the layers into the pom.xml, pointing out to the configuration file layers.xml where we define how the folders, files, and resources are separated into different layers and the order of them. &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${springboot.version}&lt;/version&gt; &lt;configuration&gt; &lt;layers&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;configuration&gt;${project.basedir}/src/layers.xml&lt;/configuration&gt; &lt;/layers&gt; &lt;image&gt; &lt;name&gt;${spring-boot.build-image.name}&lt;/name&gt; &lt;/image&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; In this layers.xml file, we define the following layers in this order (the first four are default ones, adding custom kjars layer at the end, as it is the more likely to change during application lifetime): * dependencies any dependency whose version does not contain SNAPSHOT. * spring-boot-loader for the loader classes. * snapshot-dependencies dependencies whose version contains SNAPSHOT. * application for local module dependencies, application classes, and resources but KJARs. * kjars for all the KJARs in the BOOT-INF/classes/KIE-INF folder, which were separated during package-dependencies-kjar goal execution. &lt;layers xmlns="http://www.springframework.org/schema/boot/layers" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/boot/layers https://www.springframework.org/schema/boot/layers/layers-2.5.xsd"&gt; &lt;application&gt; &lt;into layer="spring-boot-loader"&gt; &lt;include&gt;org/springframework/boot/loader/**&lt;/include&gt; &lt;/into&gt; &lt;into layer="kjars"&gt; &lt;include&gt;BOOT-INF/classes/KIE-INF/**&lt;/include&gt; &lt;/into&gt; &lt;into layer="application" /&gt; &lt;/application&gt; &lt;dependencies&gt; &lt;into layer="snapshot-dependencies"&gt; &lt;include&gt;*:*:*SNAPSHOT&lt;/include&gt; &lt;/into&gt; &lt;into layer="dependencies"&gt; &lt;includeModuleDependencies /&gt; &lt;/into&gt; &lt;/dependencies&gt; &lt;layerOrder&gt; &lt;layer&gt;dependencies&lt;/layer&gt; &lt;layer&gt;spring-boot-loader&lt;/layer&gt; &lt;layer&gt;snapshot-dependencies&lt;/layer&gt; &lt;layer&gt;application&lt;/layer&gt; &lt;layer&gt;kjars&lt;/layer&gt; &lt;/layerOrder&gt; &lt;/layers&gt; Finally, once we have the configuration of the pom.xml and layers.xml set up, we may launch the package process by invoking to the maven command: $ mvn clean package -DskipTests SPRING BOOT KIE SERVER LAYERS INSPECT We can check out the result of this layering process using the property jarmode=layertools with the list argument for the generated fat-jar: $ java -Djarmode=layertools -jar target/immutable-springboot-kie-server-1.0.0.jar list dependencies spring-boot-loader snapshot-dependencies application kjars Our custom kjars layer is the last one as defined in the layerOrder node of layers.xml file. Another interesting file we may check out is the layers.idx where packaging information (separated folders and layer order) is stored. $ cat application/BOOT-INF/layers.idx - "dependencies": - "BOOT-INF/lib/" - "spring-boot-loader": - "org/" - "snapshot-dependencies": - "application": - "BOOT-INF/classes/application.properties" - "BOOT-INF/classes/org/" - "BOOT-INF/classes/quartz-db.properties" - "BOOT-INF/classpath.idx" - "BOOT-INF/layers.idx" - "META-INF/" - "kjars": - "BOOT-INF/classes/KIE-INF/" Moreover, we can extract the layers again using the property jarmode=layertools with the extract argument for the generated fat-jar: $ java -Djarmode=layertools -jar target/immutable-springboot-kie-server-1.0.0.jar extract $ tree kjars kjars └── BOOT-INF └── classes └── KIE-INF └── lib ├── kjar-sample-1.0.0.jar ├── kjar-sample-1.1.0.jar └── other-kjar-1.0.0.jar 4 directories, 3 files We can use this utility in the Dockerfile for easily extracting the layers of the fat-jar and dockerize our immutable Spring Boot Kie Server application. LAYERED DOCKERFILE We can define a multi-stage Dockerfile to take advantage of this layering and build the image: FROM openjdk:8-slim as builder WORKDIR application ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} immutable-springboot-kie-server-1.0.0.jar RUN java -Djarmode=layertools -jar immutable-springboot-kie-server-1.0.0.jar extract FROM openjdk:8-slim WORKDIR application COPY --from=builder application/dependencies/ ./ COPY --from=builder application/spring-boot-loader/ ./ COPY --from=builder application/snapshot-dependencies/ ./ COPY --from=builder application/application/ ./ COPY --from=builder application/kjars/ ./ ENTRYPOINT ["java", "org.springframework.boot.loader.JarLauncher"] TIP: You may consider any other layering depending on the likelihood of changes for the grouped libraries, code, and resources. Happy layering!! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hYH2jJwUYUQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Gonzalo Muñoz Fernández</dc:creator><feedburner:origLink>https://blog.kie.org/2021/06/custom-layered-immutable-spring-boot-kie-server.html</feedburner:origLink></entry><entry><title>Join the Red Hat team at OpenJS World 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/a2xNf6CngUk/join-red-hat-team-openjs-world-2021" /><author><name>Michael Dawson</name></author><id>066f25a4-7a0e-43d1-be62-1bfd9f4ef265</id><updated>2021-06-01T12:30:00Z</updated><published>2021-06-01T12:30:00Z</published><summary type="html">&lt;p&gt;Red Hat is excited to be back at the &lt;a href="https://openjsf.org/openjs-world-2021/"&gt;OpenJS World&lt;/a&gt; conference again this year. We look forward to connecting with you to explore the impact &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; are having on technologies of all kinds, especially in the area of cloud-native development.&lt;/p&gt; &lt;p&gt;Many developers and community contributors from the Red Hat and IBM teams will deliver talks and participate in conference events. We hope to see you there!&lt;/p&gt; &lt;h2&gt;OpenJS World keynotes, sessions, and labs hosted by Red Hat and IBM&lt;/h2&gt; &lt;p&gt;This year's virtual event includes many great topics and speakers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Keynotes:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Wednesday, June 2 at 9:00 a.m. PDT: &lt;a href="https://openjsworld2021.sched.com/event/j00p/welcome-keynote-the-roaring-twenties-for-javascript-robin-bender-ginn-executive-director-openjs-foundation-todd-moore-vp-of-open-technology-and-developer-advocacy-ibm"&gt;The Roaring Twenties for JavaScript&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Co-presented by Todd Moore (Vice President, Open Technology and Developer Advocacy, IBM)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Wednesday, June 2 at 10:25 a.m. PDT: &lt;a href="https://openjsworld2021.sched.com/event/j06C/keynote-open-open-source-and-making-great-places-for-collaboration-joe-sepi-open-source-engineer-advcoate-ibm-michael-dawson-nodejs-lead-for-ibm-and-red-hat-beth-griggs-senior-software-engineer-red-hat"&gt;Open Open Source and Making Great Places for Collaboration&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Joe Sepi (Open Source Engineer &amp; Advocate, IBM), Michael Dawson (Node.js lead, IBM and Red Hat), and Bethany Griggs (Senior Software Engineer, Red Hat)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Sessions:&lt;/strong&gt; Available together June 2 at 1:00 p.m. PDT.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/izzz/internet-of-things-iot-with-node-both-practical-and-fun-jesse-gorzinski-ibm-michael-dawson-red-hat"&gt;Internet of Things (IoT) with Node: Both Practical and Fun!&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Jesse Gorzinski (IBM) and Michael Dawson (Red Hat)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/j002/nodejs-diagnostic-best-practices-gireesh-punathil-ibm-india-mary-marchini-netflix"&gt;Node.js Diagnostic Best Practices&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Co-presented by Gireesh Punathil (IBM India)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/izze/panel-nodejs-package-maintenance-working-group-year-3-glenn-hinks-american-express-bethany-griggs-red-hat-darcy-clarke-github-dominykas-blyze-nearform-rodion-abdurakhimov-aspire-global"&gt;Node.js Package Maintenance Working Group: Year 3&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Panel discussion featuring Bethany Griggs (Red Hat)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/j06f/responsible-coding-for-a-better-future-lucile-jerber-stephane-rodet-ibm"&gt;Responsible Coding for a Better Future&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Lucile Jerber and Stephane Rodet (IBM)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/j00K/take-a-trip-through-jslandia-joe-sepi-ibm-jory-burson-linux-foundation"&gt;Take a Trip through JSLandia&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Co-presented by Joe Sepi (IBM)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/j005/application-modernization-with-camel-javascript-and-openshift-ip-sam-wuxin-zeng-red-hat"&gt;Application Modernization with Camel JavaScript and OpenShift&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Ip Sam and Wuxin Zeng (Red Hat)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/izzt/nodejs-deep-debugging-gireesh-punathil-ibm-india"&gt;Node.js Deep Debugging&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Gireesh Punathil (IBM India)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/izzD/nodejs-the-new-and-the-experimental-bethany-griggs-red-hat"&gt;Node.js: The New and the Experimental&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Bethany Griggs (Red Hat)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openjsworld2021.sched.com/event/izyR/cloud-native-landscape-for-nodejs-developers-upkar-lidder-ibm"&gt;Cloud Native Landscape for Node.js Developers&lt;/a&gt; &lt;ul&gt;&lt;li&gt;Presented by Upkar Lidder (IBM)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We are also hosting a set of labs on June 3. For more information, check out &lt;a href="https://developer.ibm.com/conferences/openjs-world/"&gt;OpenJS World: IBM Day of Workshops&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Explore more Node.js resources&lt;/h2&gt; &lt;p&gt;If you want to learn more about Red Hat and IBM’s involvement in the Node.js community and what we are working on, check out our topic pages at &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Red Hat Developer&lt;/a&gt; and &lt;a href="https://developer.ibm.com/languages/node-js/"&gt;IBM Developer&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/01/join-red-hat-team-openjs-world-2021" title="Join the Red Hat team at OpenJS World 2021"&gt;Join the Red Hat team at OpenJS World 2021&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/a2xNf6CngUk" height="1" width="1" alt=""/&gt;</summary><dc:creator>Michael Dawson</dc:creator><dc:date>2021-06-01T12:30:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/01/join-red-hat-team-openjs-world-2021</feedburner:origLink></entry><entry><title>How to create a better front-end developer experience</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lVMpKj-n8rc/how-create-better-front-end-developer-experience" /><author><name>Zackary Allen</name></author><id>9a17ebc5-2eca-4ea4-bdeb-3443661d9df2</id><updated>2021-06-01T07:00:00Z</updated><published>2021-06-01T07:00:00Z</published><summary type="html">&lt;p&gt;Who are the first users of a new feature or new application? If you think they are customers, think again.&lt;/p&gt; &lt;p&gt;The first users are actually the front-end developers, and their experience testing those new applications and features makes your first &lt;a href="https://developers.redhat.com/blog/category/uiux/"&gt;user experience&lt;/a&gt; (UX). If your front-end developers have a smooth experience developing new products, your users will almost always have a smooth experience using them.&lt;/p&gt; &lt;p&gt;Take developing a form using React, for example. If developers are able to develop the form without any difficulty, it will likely be a positive experience for the customer as well. The reason? The developer had to fill out the form to test it. If tweaking the form takes one second but filling it out takes one minute, the developer will probably find a way to reduce the feedback loop. It might be reduced through technical means by integrating with browsers that autofill address fields, or by advising the design team that the form could be split up so it can be more modularly tweaked and tested. Whatever the case, &lt;em&gt;developers tend to write software consistent with their tools&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;That's why UX design teams should strive to not only improve the end-user experience in their products, but also streamline the experience of the developers who build them. &lt;em&gt;Developer experience&lt;/em&gt; refers to the workflow developers move through as they write, update, and maintain code for each release. From local build tooling to shared workflows and shared deployments, strong developer experience paves the path for solid UX.&lt;/p&gt; &lt;p&gt;In this article, we'll explore common pain points that can complicate the development process and how to address them to foster better developer experiences.&lt;/p&gt; &lt;h2&gt;What makes a good developer experience?&lt;/h2&gt; &lt;p&gt;Front-end developers want to be able to write code to add features in an environment that closely resembles what users will encounter in the end product. After committing and pushing their code changes, they'd typically like to run tests to make sure their changes don't break anything unexpectedly. Beyond test validation, front-end developers might want to validate new features by sharing a link with stakeholders. Once their changes meet the stakeholders' criteria, developers want their code to make its way into the central repository and then to end users. Sometimes the new features reach end users as soon as the changes are merged, and sometimes that transition happens on a time-based schedule.&lt;/p&gt; &lt;p&gt;A strong front-end developer experience moves smoothly through each of these phases. Unfortunately, few front-end development experiences are seamless.&lt;/p&gt; &lt;h2&gt;Common pain points for front-end developers&lt;/h2&gt; &lt;p&gt;Common front-end development pain points span three main areas: environments, testing, and releases. Often, front-end development environments lack key features that end-user environments have, such as authorization or live data. Frequently, networking pieces are missing or need to be properly proxied to test these environments. And speaking of tests, they don't write, run, or analyze themselves!&lt;/p&gt; &lt;p&gt;There's no silver bullet that solves any of these problems, yet front-end developers spend a significant portion of their time on them. Let's examine each of these pain points in detail, and look at methods front-end developers and &lt;a href="https://developers.redhat.com/topics/devops/"&gt;DevOps&lt;/a&gt; engineers can use to help solve them.&lt;/p&gt; &lt;h3&gt;Pain point #1: Environments&lt;/h3&gt; &lt;p&gt;While most front ends start standalone, few continue on their own because they usually need certain back-end services. Back-end developers can develop without a front end, but front-end developers certainly can’t develop without a back end. In general, there are three solutions to running back-end services to use against a local front end:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Run the back-end services locally (see Figure 1).&lt;/li&gt; &lt;li&gt;Use a shared back-end deployment.&lt;/li&gt; &lt;li&gt;Use mock data in your front end.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Solutions one and three allow offline development. While an internet connection is a given nowadays, offline development is still a better experience than online development for developers who travel or live in places that experience intermittent outages due to weather. Offline development also doesn't require a VPN, which can be difficult to set up on certain devices.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/hot-reloading.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/hot-reloading.gif" width="2607" height="1387" alt="Text editor and browser open side-by-side" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Hot-reloading changes with Webpack on a local cloud.redhat.com environment with back-end services running locally.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Solutions one and two require an extra step to run a command before developing the front end. The local back end might need additional configuration (like a database), and a shared back end might require a proxy. This leads to a worse developer experience but can help catch back-end bugs earlier than mocking the data might.&lt;/p&gt; &lt;p&gt;The third solution provides the best overall developer experience but requires the most work, because each back-end endpoint must be spoofed to return mocked data. This requires front-end and back-end developers working together to create mocks.&lt;/p&gt; &lt;p&gt;The first solution provides the best back-end developer experience because they can test local back-end changes against a front end.&lt;/p&gt; &lt;p&gt;The environment is the first pain point that needs solving for front-end developers, and is generally the hardest of the common pain points to get right. Being able to work offline is a good experience. Not having to run a back end to make front end changes is a &lt;em&gt;great&lt;/em&gt; experience.&lt;/p&gt; &lt;h3&gt;Pain point #2: Testing&lt;/h3&gt; &lt;p&gt;Most front ends don't start out needing tests. However, as they grow, it's important that the user experience doesn't degrade when adding new features or fixing unrelated bugs. Writing, running, and reporting tests can be painful. Let's take a look at how to ease the testing process across two different front-end testing categories: unit tests and integration tests.&lt;/p&gt; &lt;h4&gt;Unit tests&lt;/h4&gt; &lt;p&gt;The best way to make test writing easier is only to write tests that are important. Adding automatic snapshot tests for custom components can help catch bugs. After that, spend time testing event and state interactions. When developing tests, most tools have a watch mode that can be used to run tests only when the test files change.&lt;/p&gt; &lt;h4&gt;Integration tests&lt;/h4&gt; &lt;p&gt;With certain frameworks, it’s possible to &lt;a href="https://chrome.google.com/webstore/detail/cypress-recorder/glcapdcacdfkokcmicllhcjigeodacab?hl=en-US"&gt;record user interactions&lt;/a&gt; on a webpage to avoid coding the test cases yourself. For large test suites, tools like &lt;a href="https://www.browserstack.com/"&gt;BrowserStack&lt;/a&gt; offer a grid of 10 concurrent runners for free and open source accounts.&lt;/p&gt; &lt;h4&gt;Reporting tests&lt;/h4&gt; &lt;p&gt;There are dozens of reporting formats, but HTML reports that can be uploaded to a pull request (PR) like the &lt;a href="https://github.com/patternfly/patternfly-react/pull/5524"&gt;one shown in Figure 2&lt;/a&gt; are usually best. For unit tests, tools like &lt;a href="http://codecov.io"&gt;Codecov&lt;/a&gt; can help maintain a certain coverage percentage as well.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/a11y-report.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/a11y-report.png?itok=4Rc7RPoX" width="600" height="345" alt="An HTML accessibility report for an @patternfly/react-core pull request " typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: An HTML accessibility report for a pull request complete with screenshots.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Pain point #3: Releasing&lt;/h3&gt; &lt;p&gt;When opening a pull request, it’s often useful to share changes with designers and other developers in the form of a PR preview. If you just need to host static files, services like &lt;a href="https://www.netlify.com/"&gt;Netlify&lt;/a&gt; can work with minimal configuration, or a service like &lt;a href="https://surge.sh/"&gt;Surge&lt;/a&gt; can work with an existing continuous integration (CI) system. For sites that also need a back end, &lt;a href="https://vercel.com/"&gt;Vercel&lt;/a&gt; and &lt;a href="https://www.heroku.com/"&gt;Heroku&lt;/a&gt; have free tiers sufficient for most deployments.&lt;/p&gt; &lt;p&gt;For releasing, tools like &lt;a href="https://github.com/semantic-release/semantic-release"&gt;semantic-release&lt;/a&gt; for normal repos and &lt;a href="https://lerna.js.org/"&gt;Lerna&lt;/a&gt; for monorepos (see Figure 3) can release to Git, GitHub, or Node Package Manager (npm) on every push to the main branch of your repository. Of course, there's always the option to write your own bash script for complete flexibility.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/patternfly-npm.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/patternfly-npm.png?itok=qIk-spJt" width="600" height="294" alt="Lerna auto-releasing @patternfly/react-* packages to npm from a merged pull request." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Lerna auto-releasing @patternfly/react-* packages to npm from a merged pull request.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion: Developer experience is user experience&lt;/h2&gt; &lt;p&gt;Improving the front-end developer experience lowers the cost of front-end development and enhances the overall user experience. Addressing front-end developer pain points will look different for every project, but the payoff is the same: A smoother experience for developers carries through to your end users.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/01/how-create-better-front-end-developer-experience" title="How to create a better front-end developer experience "&gt;How to create a better front-end developer experience &lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lVMpKj-n8rc" height="1" width="1" alt=""/&gt;</summary><dc:creator>Zackary Allen</dc:creator><dc:date>2021-06-01T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/01/how-create-better-front-end-developer-experience</feedburner:origLink></entry><entry><title type="html">Retail data framework - Common architectural elements</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/xXFiSsZFv0A/retail-data-framework-common-architectural-elements.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/zejVwlU1WVc/retail-data-framework-common-architectural-elements.html</id><updated>2021-06-01T05:00:00Z</updated><content type="html">Part 2 - Common architectural elements  In our  from this series we introduced a use case around the data framework for retail stores. The process was laid out how we've approached the use case and how portfolio solutions are the base for researching a generic architectural blueprint.  The only thing left to cover was the order in which you'll be led through the blueprint details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. This will start our journey into the logical elements that make up the real-time stock control architecture blueprint. BLUEPRINTS REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common blueprint that was uncovered researching those solutions. It's our intent to provide a blueprint that provides guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architectural blueprint, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. FROM SPECIFIC TO GENERIC Before diving in to the common elements, it might be nice to understand that this is not a catch all for every possible supply chain integration solution. It's a collection of identified elements that we've uncovered in multiple customer implementations. These elements presented here are then the generic common architectural elements that we've identified and collected in to the generic architectural blueprint.  It's our intent to provide a blueprint for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.  It's our job here to describe the architectural blueprint generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architectural blueprint. There are many ways to represent each element, but we've chosen some icons, text and colours that we hope are going to make it all easy to absorb. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in my research. DATA INTEGRATION PLATFORM The logical view splits this solution space into several identifiable platforms where the retail data framework is managed. It takes all three of these platforms to ensure that the data is collected, integrated with the various blueprints external to the framework, processed, validated, stored, data science is applied for insights, and exposed back out to the entire retail organisation. The first we'll look at is the data integration platform where the main action takes place with the retail data framework. Here there are integration microservices and data integration microservices to provide integration with the core platform, data science platform, and storage services.  Another important set of elements found in this platform are messaging and event processing. Both are essential elements to ensure microservice communications and message transformation within the architecture. To help with data performance, availability, and management there are data caching microservices and data virtualisation microservices.  Next up, process automation is used to capture processes within the retail organisation, manage the processing and validation of data in a structured traceable manner. The business automation microservices capture all of these processes and ensure proper monitoring of the compliance and regulatory rules for the entire retail organisation.  Finally, ensuring that the fronting web applications have good visual data representations requires that all microservices are only accessed by authenticated and authorised parties. This is taken care of through the use of an API management element.  You can sense that this data integration platform contains the elements focusing on microservice deployments which lend themselves to a cloud-native development process using containers and container platforms.  CORE PLATFORM A core platform focusing on security and compliance requires the hosting of retail organisation wide tooling. These are not specifically called out and can be any number of core services or systems hosted within the retail organisation or outside in the form of Software as a Service (SaaS) solutions. This platform hosts a set of four elements that each support the organisation, starting with compliance and regulatory tooling. This is not the rules mentioned in the previous section, but the tooling backing the development, deployment, and maintenance of the compliance and regulatory rules.  There should be some form of auditing tooling and governance tooling used to ensure data and the services used to support the retail data framework are properly monitored in their application. Finally, the authentication and authorisation tooling is the central system used to plug in organisational wide access to the right parties within the retail data framework. DATA SCIENCE PLATFORM Any retail organisation working in their markets has a vast interest in the behaviour patterns of their customers. At the very least they are using the most basic data science elements, and in advanced cases, they are leveraging all forms of data science to advance their market positions. In the data science platform we find the more classical business intelligence tooling, often an externally hosted system noted here with a private cloud icon. Providing views and cuts of the mass data collected in retail organisations is the fundamental function of this element.  The advanced use of not just analytics on their data, but applying more sophisticated technologies like data science (AI / ML) allows retail organisations to gain advantageous insights into customers, trends, products, sales, and other retail activities that raw data analysis can't provide. Finally, there is data visualisation tooling that provides clear visibility to the consumers of the data and analysis generated from the other elements on this platform. STORAGE SERVICES The storage services uncovered in this solution space was a fairly diverse and more high level than the usually noted storage elements found in our architecture blueprints. As these storage needs are data focused and organisation wide solutions, you find all the major technologies in the data world applied here, such as data lakes, data warehousing, data hubs, and data marts. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture blueprint for the retail data framework use case.  An overview of this series on retail data framework portfolio architecture blueprint: 1. 2. 3. Example data architecture Catch up on any past articles you missed by following any published links above. Next in this series, taking a look at an example data framework architecture for this blueprint. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/xXFiSsZFv0A" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/zejVwlU1WVc/retail-data-framework-common-architectural-elements.html</feedburner:origLink></entry><entry><title type="html">Eclipse Vert.x 4.1.0 released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/-N-Iek-NXNQ/eclipse-vert-x-4-1-0" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-1-0</id><updated>2021-06-01T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.1.0 has just been released. It comes with a set of new exciting features!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/-N-Iek-NXNQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Julien Viet</dc:creator><feedburner:origLink>https://vertx.io/blog/eclipse-vert-x-4-1-0</feedburner:origLink></entry><entry><title type="html">Infinispan (Red Hat Data Grid) featured in Red Hat Developers</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lF7ai7DIYvI/infinispan-redhat-summit-quarkus" /><author><name>Katia Aresti</name></author><id>https://infinispan.org/blog/2021/05/31/infinispan-redhat-summit-quarkus</id><updated>2021-05-31T12:00:00Z</updated><content type="html">Dear Infinispan Community, The Infinispan team is pleased to share an article published over on the Red Hat Developer blog. Part one of a two-part series of articles, this blog post focuses on how Data Grid, which is built on Infinispan, was used to create a leaderboard for an online Battleship game that featured at this year’s Red Hat Summit Keynote. You can read the blog post Technologies featured in our keynote demo: * Quarkus and Infinispan Client Extension * Additional Quarkus extensions: RestEasy, Websockets, Scheduler * Infinispan Query * Infinispan Cross-Site Replication * Infinispan Kubernetes Operator Enjoy your reading!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lF7ai7DIYvI" height="1" width="1" alt=""/&gt;</content><dc:creator>Katia Aresti</dc:creator><feedburner:origLink>https://infinispan.org/blog/2021/05/31/infinispan-redhat-summit-quarkus</feedburner:origLink></entry><entry><title>Learn Quarkus faster with quick starts in the Developer Sandbox for Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/FSoQUzD7WPk/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift" /><author><name>Daniel Oh</name></author><id>9e2c023a-1daf-4ba7-b00d-1bf2469de902</id><updated>2021-05-31T07:00:00Z</updated><published>2021-05-31T07:00:00Z</published><summary type="html">&lt;p&gt;Java developers are usually required to take many actions before we can begin developing and deploying cloud-native &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. First, we have to configure everything from the integrated development environment (IDE) to build tools such as &lt;a href="https://maven.apache.org"&gt;Maven&lt;/a&gt; or &lt;a href="https://gradle.org"&gt;Gradle&lt;/a&gt;. We also need to configure the command-line tools used for containerization and generating the Kubernetes manifest. If we don’t want to spin up a Kubernetes cluster locally, we also must connect to a remote Kubernetes cluster for continuous testing and deployment.&lt;/p&gt; &lt;p&gt;Developers should spend less time on configuration and more time accelerating the inner-loop development cycle of building, testing, and deploying our applications. Ideally, we should be able to continuously develop applications in a pre-configured Kubernetes environment.&lt;/p&gt; &lt;p&gt;This article is a guide to configuring Java applications using &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt; quick starts in the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;. As you'll see, using quick starts in the developer sandbox lets you focus on the inner loop of development, without needing to configure the Kubernetes cluster or development tools.&lt;/p&gt; &lt;p&gt;Developers using the developer sandbox have access to a shared, multi-tenant &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift 4&lt;/a&gt; cluster with a set of pre-installed developer tools such as a web-based IDE and &lt;a href="https://developers.redhat.com/products/codeready-workspaces/overview"&gt;Red Hat CodeReady Workspaces&lt;/a&gt;. You can get started in less than five minutes with a free Red Hat developer account. To learn more about the developer sandbox, click &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Step 1: Launch your developer sandbox&lt;/h2&gt; &lt;p&gt;Assuming you have set up and signed into your Red Hat developer account, you can start your OpenShift environment in the developer sandbox. Go to the &lt;a href="https://developers.redhat.com/developer-sandbox/get-started"&gt;Get started in the Sandbox&lt;/a&gt; page, then click on &lt;strong&gt;Start using your Sandbox&lt;/strong&gt;, as shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="The 'Get started in the Sandbox' option is selected." data-entity-type="file" data-entity-uuid="b962a4e4-1ead-4fb6-a35c-4dbab2a3f534" height="380" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.21.07%20PM.png" width="593" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: Start using your developer sandbox.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You must choose &lt;strong&gt;DevSandbox&lt;/strong&gt; to log into the cluster.&lt;/p&gt; &lt;h2&gt;Step 2: Explore Quarkus quick starts&lt;/h2&gt; &lt;p&gt;Once you log in, you will arrive at the OpenShift topology view. Click &lt;strong&gt;View all Quick Starts&lt;/strong&gt; then enter a search for "Quarkus." You will see the three quick starts shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Enter a search for 'Quarkus' to view the three available Quarkus quick starts." data-entity-type="file" data-entity-uuid="765f40f4-ee54-4177-a1b0-8453ba53c653" height="197" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.22.49%20PM.png" width="586" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Quarkus quick starts in the OpenShift topology view.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Step 3: Open your first Quarkus quick start&lt;/h2&gt; &lt;p&gt;Select the "Get started with Quarkus using S2I" quick start shown in Figure 2. This 10-minute quick start takes you through the six tasks to create and deploy a Quarkus application on OpenShift using a source-to-image (S2I) approach:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create the Quarkus application.&lt;/li&gt; &lt;li&gt;View the build status.&lt;/li&gt; &lt;li&gt;View the associated Git repository.&lt;/li&gt; &lt;li&gt;View the pod status.&lt;/li&gt; &lt;li&gt;Change the deployment icon to Quarkus.&lt;/li&gt; &lt;li&gt;Run the Quarkus application.&lt;/li&gt; &lt;/ol&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The Quarkus project is automatically created for you in the sandbox environment, so you can skip the first task and complete the quick start faster.&lt;/p&gt; &lt;h2&gt;Step 4: Run the 'Get started with Quarkus using S2I' quick start&lt;/h2&gt; &lt;p&gt;When you are ready, click &lt;strong&gt;Start tour&lt;/strong&gt;, as shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="The six tasks are shown, along with the option to start the tour." data-entity-type="file" data-entity-uuid="66c5f6e0-cc7b-499d-ae69-7ab23292c9db" height="424" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.24.16%20PM_1.png" width="456" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3: Start the tour.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The tour guides you through the step-by-step instructions to complete the tasks in this quick start. You can skip the first task because it has already been done automatically for you, as shown in Figure 4.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="The project has already been created automatically." data-entity-type="file" data-entity-uuid="b67d70e4-fa40-4bc5-8b76-1eb007e04ea0" height="347" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.25.30%20PM.png" width="588" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4: The Quarkus application has already been created.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;When you complete each step, click &lt;strong&gt;Next&lt;/strong&gt; to verify your work. If you have accomplished the task without any issues, click &lt;strong&gt;Yes&lt;/strong&gt;, then click &lt;strong&gt;Next&lt;/strong&gt; again.&lt;/p&gt; &lt;p&gt;Most tasks in this quick start are self-explanatory, but a couple of them are worth exploring.&lt;/p&gt; &lt;h3&gt;Task 3: View the associated Git repository&lt;/h3&gt; &lt;p&gt;The developer sandbox lets developers change application code directly using CodeReady Workspaces instead of navigating to the Git repository from a local IDE. This makes pre-configuring the application easier, as I mentioned earlier. You also can run the "getting started" application in Quarkus development mode, which lets you use Quarkus's live coding feature for continuous development.&lt;/p&gt; &lt;p&gt;When you click the &lt;strong&gt;CodeReady Workspaces&lt;/strong&gt; icon in the bottom-right quadrant of the &lt;strong&gt;quarkus-quickstarts&lt;/strong&gt; deployment, it brings you to CodeReady Workspaces, as shown in Figure 5.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="CodeReady Workspaces opens in the console." data-entity-type="file" data-entity-uuid="0b229b5c-9059-418f-8844-5af1a0bda0eb" height="317" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.27.16%20PM.png" width="569" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 5: View your code in CodeReady Workspaces.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Task 6: Run the Quarkus application&lt;/h3&gt; &lt;p&gt;In Task 6, you can access the Quarkus application's REST API. Click the external link icon to open the URL and run the application in a new browser tab, as shown in Figure 6.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Run the Quarkus application from the REST API." data-entity-type="file" data-entity-uuid="aefc7785-9e09-48bb-bffd-c263a105b841" height="311" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.28.12%20PM.png" width="570" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 6: Open the Quarkus application URL.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Step 5: Finish the quick start&lt;/h2&gt; &lt;p&gt;When you have completed all six tasks you will see the green checkmarks shown in Figure 7.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Each task has a green checkmark beside it." data-entity-type="file" data-entity-uuid="e9e9ae91-9c5e-497f-b6d5-2ca8d8c7b1f5" height="462" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-05-24%20at%205.28.56%20PM_0.png" width="527" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 7: The green checkmarks indicate that all tasks have been completed.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Great job! Now, you can tour another quick start or repeat this one as a learning practice.&lt;/p&gt; &lt;h2&gt;Watch the Quarkus quick starts video demonstration&lt;/h2&gt; &lt;p&gt;If you want further instruction, this video demonstration guides you through two of the three available Quarkus quick starts: "Get started with Quarkus using S2I" and "Get started with Quarkus using a Helm chart."&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, you've learned how much faster you can get started with Quarkus application development and deployment using Quarkus quick starts in the Developer Sandbox for Red Hat OpenShift. The sandbox is free for all business application developers who are interested in cloud-native microservices development using Quarkus. The sandbox lets you use a modern web-based IDE for development and deploy your cloud-native microservices applications to a Red Hat OpenShift cluster seamlessly. Use the self-service learning portal &lt;a href="https://developers.redhat.com/courses/quarkus"&gt;here&lt;/a&gt; to learn more about Java application development with Quarkus.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift" title="Learn Quarkus faster with quick starts in the Developer Sandbox for Red Hat OpenShift"&gt;Learn Quarkus faster with quick starts in the Developer Sandbox for Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/FSoQUzD7WPk" height="1" width="1" alt=""/&gt;</summary><dc:creator>Daniel Oh</dc:creator><dc:date>2021-05-31T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift</feedburner:origLink></entry><entry><title type="html">This Week in JBoss - 31 May 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ApC_h-WHvGM/weekly-2021-05-31.html" /><category term="quarkus" /><category term="wildfly" /><category term="keycloak" /><category term="kogito" /><category term="infinispan" /><category term="camel" /><category term="jgroups" /><category term="vert.x" /><author><name>Don Naro</name><uri>https://www.jboss.org/people/don-naro</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-05-31.html</id><updated>2021-05-31T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, wildfly, keycloak, kogito, infinispan, camel, jgroups, vert.x"&gt; &lt;h1&gt;This Week in JBoss - 31 May 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hello! Welcome to another edition of the JBoss Editorial that brings you news and updates from our community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_hello_again"&gt;Hello again&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;To our community and all our readers,&lt;/p&gt; &lt;p&gt;I’d like to start this edition with a sincere and frank apology on behalf of the editorial team for the posts we missed in the last month.&lt;/p&gt; &lt;p&gt;There’s been a lot of awesome content that our community has shared and multiple project releases packed with useful new features and clever enhancements. We’re long overdue in highlighting and celebrating all the great work that JBoss teams are doing, not to mention all the brilliant work of our evangelists and other contributors.&lt;/p&gt; &lt;p&gt;It’s been a busy past few weeks and we’ve got a lot of great articles and releases to catch up on, so let’s go.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Let’s start things off with congrats to all the teams on their hard work!&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://belaban.blogspot.com/2021/05/jgroups-517-released.html/"&gt;JGroups 5.1.7&lt;/a&gt; is released. Congrats, Bela!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-1-13-6-final-released/"&gt;Quarkus 1.13.6.Final&lt;/a&gt; has shipped, which is the latest in a recent series of updates to version 1.13.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-0-0-cr2-released/"&gt;Quarkus 2 CR2&lt;/a&gt; is out!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-1-CR2-released/"&gt;Vert.x 4.1.0.CR2&lt;/a&gt; is here, right on the heels of the beta and CR1 releases.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://infinispan.org/blog/2021/05/07/infinispan-12-1-2-final"&gt;Infinispan 12.1.2 Final&lt;/a&gt; is available for download so go grab it.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2021/05/keycloak-1301-released"&gt;Keycloak 13.0.1&lt;/a&gt; is out!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2021/05/kogito-tooling-0-10-0-released.html"&gt;Kogito Tooling 0.10.0&lt;/a&gt; has launched!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://bytemanblog.blogspot.com/2021/05/byteman-4015-has-been-released.html]"&gt;Byteman 4.0.15&lt;/a&gt; is now available!&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_quarkus_2_ama"&gt;Quarkus 2 AMA&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;With the recent release of Quarkus CR2, I’m sure we’re all anticipating Quarkus 2.0 GA. As part of that release, Max and the rest of the Quarkus team are taking questions that they will answer on an episode of Quarkus Insights. Use the &lt;code&gt;#quarkusinsights&lt;/code&gt; tag to submit a question via social media and tune in to &lt;a href="https://www.youtube.com/watch?v=ETTMBWEBfLY"&gt;Quarkus Insights #51: Q &amp;#38; A - Part II&lt;/a&gt; on June 2 to hear your questions answered.&lt;/p&gt; &lt;p&gt;Follow the social media links to post your question to the Quarkus team in their post, &lt;a href="https://quarkus.io/blog/quarkus-insights-qanda2/"&gt;About to release Quarkus 2 - ask us anything!&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_from_the_community"&gt;From the community&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Jeff Mesnil has authored a very helpful and detailed look at building and deploying WildFly applications on OpenShift using Helm Charts in his post, &lt;a href="https://www.wildfly.org/news/2021/05/05/helm-charts-for-wildfly/"&gt;Helm Chart for WildFly&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Over on the Infinispan blog, Ryan Emerson has shared some details about the CLI compiled to a native image in &lt;a href="https://infinispan.org/blog/2021/05/21/infinispan-cli-image"&gt;Infinispan Native CLI&lt;/a&gt;, which is well worth a read. You should also try downloading the latest Infinispan 12 server version and taking it for a spin with the native CLI!&lt;/p&gt; &lt;p&gt;Another recent one from the Infinispan team comes from Katia Aresti who, along with Ryan Emerson, explains how they &lt;a href="https://developers.redhat.com/articles/2021/05/28/building-real-time-leaderboard-red-hat-data-grid-and-quarkus-hybrid-kubernetes"&gt;built a real-time leaderboard using Data Grid and several Quarkus extensions&lt;/a&gt; to add some magic to this year’s Red Hat Summit Keynote.&lt;/p&gt; &lt;p&gt;Jacopo Rota on the Kogito blog explains how to &lt;a href="https://blog.kie.org/2021/05/getting-started-with-trustyai-in-only-15-minutes.html"&gt;how to deploy a Kogito service together with the TrustyAI infrastructure on an OpenShift cluster in only 15 minutes&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Have you been wanting to find out more about Shenandoah GC? Well, you should dive right in and check out Roman Kennke’s informative post, &lt;a href="https://developers.redhat.com/articles/2021/05/20/shenandoah-garbage-collection-openjdk-16-concurrent-reference-processing"&gt;Shenandoah garbage collection in OpenJDK 16: Concurrent reference processing&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Bilgin Ibryam has recently posted &lt;a href="http://www.ofbizian.com/2021/05/data-gateways-of-future.html"&gt;Data Gateways in the Cloud Native Era&lt;/a&gt; that examines how data gateway components support different use cases and offer a solution for hybrid workloads spread across multiple cloud providers.&lt;/p&gt; &lt;p&gt;Last but certainly not least is Claus Ibsen’s webinar, &lt;a href="http://www.davsclaus.com/2021/05/webinar-integrate-systems-in-age-of.html"&gt;Integrate systems in the age of Quarkus and Camel&lt;/a&gt;, that explores how the trio of Camel Quarkus, Camel K, and Kamelets simplify the work to manage and bind systems together.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_evangelists_corner"&gt;Evangelist’s corner&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Eric Schabell never disappoints and his previous series on architectural elements in a real-time stock control solution for retail was no exception. Eric rounds that series off nicely with &lt;a href="https://www.schabell.org/2021/05/real-time-stock-control-example-stock-control-architecture.html"&gt;Real-time stock control - Example stock control architecture&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Not one to rest for long, Eric Schabell launches a new series that tackles how to create a framework for accessing retail data from customers, stock, stores, and staff across multiple internal teams. I’m sure it’s going to be a brilliant series so go have a look for yourself and find out more in his post, &lt;a href="https://www.schabell.org/2021/05/retail-data-framework-architectural-introduction.html"&gt;Retail data framework - An architectural introduction&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I also really enjoyed reading through Christina and Eric’s &lt;a href="http://wei-meilin.blogspot.com/2021/05/tooling-guide-for-getting-started-with.html"&gt;Tooling guide for Getting Started with Apache Camel in 2021&lt;/a&gt;. It’s a well put together beginner’s guide to tools that can help with Camel applications.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_developers_on_film"&gt;Developers on film&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Get your popcorn ready and sit back to watch some videos from our community. Here are my top picks for this week’s editorial:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/ILl85LLj93w"&gt;Quarkus Insights #49: Why I use Quarkus for Cloud Native Apps&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/7JPm1BFcrrk"&gt;What is Serverless with Java?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/jBDmX85IjLM"&gt;No YAML! Kubernetes done the easy way&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/szza3DZlKzA"&gt;Quarkus DevServices&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/don-naro.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Don Naro&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ApC_h-WHvGM" height="1" width="1" alt=""/&gt;</content><dc:creator>Don Naro</dc:creator><feedburner:origLink>https://www.jboss.org/posts/weekly-2021-05-31.html</feedburner:origLink></entry><entry><title type="html">Data Gateways in the Cloud Native Era</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/r9Na-7W7j-Y/data-gateways-of-future.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2021/05/data-gateways-of-future.html</id><updated>2021-05-29T11:52:00Z</updated><content type="html">These days, there is a lot of excitement around 12-factor apps, microservices, and service mesh, but not so much around cloud-native data. The number of conference talks, blog posts, best practices, and purpose-built tools around cloud-native data access is relatively low. One of the main reasons for this is because most data access technologies are architectured and created in a stack that favors static environments rather than the dynamic nature of cloud environments and Kubernetes. In this article, we will explore the different categories of data gateways, from more monolithic to ones designed for the cloud and Kubernetes. We will see what are the technical challenges introduced by the Microservices architecture and how data gateways can complement API gateways to address these challenges in the Kubernetes era. APPLICATION ARCHITECTURE EVOLUTIONS Let’s start with what has been changing in the way we manage code and the data in the past decade or so. I still remember the time when I started my IT career by creating frontends with Servlets, JSP, and JSFs. In the backend, EJBs, SOAP, server-side session management, was the state of art technologies and techniques. But things changed rather quickly with the introduction of REST and popularization of Javascript. REST helped us decouple frontends from backends through a uniform interface and resource-oriented requests. It popularized stateless services and enabled response caching, by moving all client session state to clients, and so forth. This new architecture was the answer to the huge scalability demands of modern businesses. A similar change happened with the backend services through the Microservices movement. Decoupling from the frontend was not enough, and the monolithic backend had to be decoupled into bounded context enabling independent fast-paced releases. These are examples of how architectures, tools, and techniques evolved pressured by the business needs for fast software delivery of planet-scale applications. That takes us to the data layer. One of the existential motivations for microservices is having independent data sources per service. If you have microservices touching the same data, that sooner or later introduces coupling and limits independent scalability or releasing. It is not only an independent database but also a heterogeneous one, so every microservice is free to use the database type that fits its needs. Application architecture evolution brings new challenges While decoupling frontend from backend and splitting monoliths into microservices gave the desired flexibility, it created challenges not-present before. Service discovery and load balancing, network-level resilience, and observability turned into major areas of technology innovation addressed in the years that followed. Similarly, creating a database per microservice, having the freedom and technology choice of different datastores is a challenge. That shows itself more and more recently with the explosion of data and the demand for accessing data not only by the services but other real-time reporting and AI/ML needs. THE RISE OF API GATEWAYS With the increasing adoption of Microservices, it became apparent that operating such an architecture is hard. While having every microservice independent sounds great, it requires tools and practices that we didn’t need and didn’t have before. This gave rise to more advanced release strategies such as blue/green deployments, canary releases, dark launches. Then that gave rise to fault injection and automatic recovery testing. And finally, that gave rise to advanced network telemetry and tracing. All of these created a whole new layer that sits between the frontend and the backend. This layer is occupied primarily with API management gateways, service discovery, and service mesh technologies, but also with tracing components, application load balancers, and all kinds of traffic management and monitoring proxies. This even includes projects such as Knative with activation and scaling-to-zero features driven by the networking activity. With time, it became apparent that creating microservices at a fast pace, operating microservices at scale requires tooling we didn’t need before. Something that was fully handled by a single load balancer had to be replaced with a new advanced management layer. A new technology layer, a new set of practices and techniques, and a new group of users responsible were born. THE CASE FOR DATA GATEWAYS Microservices influence the data layer in two dimensions. First, it demands an independent database per microservice. From a practical implementation point of view, this can be from an independent database instance to independent schemas and logical groupings of tables. The main rule here is, only one microservice owns and touches a dataset. And all data is accessed through the APIs or Events of the owning microservice. The second way a microservices architecture influenced the data layer is through datastore proliferation. Similarly, enabling microservices to be written in different languages, this architecture allows the freedom for every microservices-based system to have a  persistence layer. With this freedom, one microservice can use a relational database, another one can use a document database, and the third microservice one uses an in-memory key-value store. While microservices allow you all that freedom, again it comes at a cost. It turns out operating a large number of datastore comes at a cost that existing tooling and practices were not prepared for. In the modern digital world, storing data in a reliable form is not enough. Data is useful when it turns into insights and for that, it has to be accessible in a controlled form by many. AI/ML experts, data scientists, business analysts, all want to dig into the data, but the application-focused microservices and their data access patterns are not  for these data-hungry demands. API and Data gateways offering similar capabilities at different layers This is where data gateways can help you. A data gateway is like an API gateway, but it understands and acts on the physical data layer rather than the networking layer. Here are a few areas where data gateways differ from API gateways. ABSTRACTION An API gateway can hide implementation endpoints and help upgrade and rollback services without affecting service consumers. Similarly, a data gateway can help abstract a physical data source, its specifics, and help alter, migrate, decommission, without affecting data consumers. SECURITY An API manager secures resource endpoints based on HTTP methods. A service mesh secures based on network connections. But none of them can understand and secure the data and its shape that is passing through them. A data gateway, on the other hand, understands the different data sources and the data model and acts on them. It can apply RBAC per data row and column, filter, obfuscate, and sanitize the individual data elements whenever necessary. This is a more fine-grained security model than networking or API level security of API gateways. SCALING API gateways can do service discovery, load-balancing, and assist the scaling of services through an orchestrator such as Kubernetes. But they cannot scale data. Data can scale only through replication and caching. Some data stores can do replication in cloud-native environments but not all. Purpose-built tools, such as , can perform change data capture from the transaction logs of data stores and enable data replication for scaling and other use cases. A data gateway, on the other hand, can speed-up access to all kinds of data sources by caching data and providing materialized views. It can understand the queries, optimize them based on the capabilities of the data source, and produce the most performant execution plan. The combination of materialized views and the stream nature of change data capture would be the ultimate data scaling technique, but there are no known cloud-native implementations of this yet. FEDERATION In API management, response composition is a common technique for aggregating data from multiple different systems. In the data space, the same technique is referred to as heterogeneous data federation. Heterogeneity is the degree of differentiation in various data sources such as network protocols, query languages, query capabilities, data models, error handling, transaction semantics, etc. A data gateway can accommodate all of these differences as a seamless, transparent data-federation layer. SCHEMA-FIRST API gateways allow contract-first service and client development with specifications such as OpenAPI. Data gateways allow schema-first data consumption based on the SQL standard. A SQL schema for data modeling is the OpenAPI equivalent of APIs. MANY SHADES OF DATA GATEWAYS In this article, I use the terms API and data gateways loosely to refer to a set of capabilities. There are many types of API gateways such as API managers, load balancers, service mesh, service registry, etc. It is similar to data gateways, where they range from huge monolithic data virtualization platforms that want to do everything, to data federation libraries, from purpose-built cloud services to end-user query tools. Let’s explore the different types of data gateways and see which fit the definition of “a cloud-native data gateway.” When I say a cloud-native data gateway, I mean a containerized first-class Kubernetes citizen. I mean a gateway that is open source, using open standards; a component that can be deployed on hybrid/multi-cloud infrastructures, work with different data sources, data formats, and applicable for many use cases. CLASSIC DATA VIRTUALIZATION PLATFORMS In the very first category of data gateways, are the traditional data virtualization platforms such as  and . While these are the most feature-laden data platforms, they tend to do too much and want to be everything from API management, to metadata management, data cataloging, environment management, deployment, configuration management, and whatnot. From an architectural point of view, they are very much like the old ESBs, but for the data layer. You may manage to put them into a container, but it is hard to put them into the cloud-native citizen category. DATABASES WITH DATA FEDERATION CAPABILITIES Another emerging trend is the fact that databases, in addition to storing data, are also starting to act as data federation gateways and allowing access to external data. For example, PostgreSQL  the ANSI SQL/MED specification for a standardized way of handling access to remote objects from SQL databases. That means remote data stores, such as SQL, NoSQL, File, LDAP, Web, Big Data, can all be accessed as if they were tables in the same PostgreSQL database. SQL/MED stands for Management of External Data, and it is also implemented by MariaDB  engine, , Teiid project discussed below, and a few . Starting in SQL Server 2019, you can now query external data sources without moving or copying the data. The  engine of SQL Server instance to process Transact-SQL queries to access external data in SQL Server, Oracle, Teradata, and MongoDB. GRAPHQL DATA BRIDGES Compared to the traditional data virtualization, this is a new category of data gateways focused around the fast web-based data access. The common thing around , , , is that they focus on GraphQL data access by offering a lightweight abstraction on top of a few data sources. This is a fast-growing category specialized for enabling rapid web-based development of data-driven applications rather than BI/AI/ML use cases. OPEN-SOURCE DATA GATEWAYS  is a schema-free SQL query engine for NoSQL databases and file systems. It offers JDBC and ODBC access to business users, analysts, and data scientists on top of data sources that don’t support such APIs. Again, having uniform SQL based access to disparate data sources is the driver. While Drill is highly scalable, it relies on Hadoop or Apache Zookeeper’s kind of infrastructure which shows its age.  is a mature data federation engine sponsored by Red Hat. It uses the SQL/MED specification for defining the virtual data models and relies on the Kubernetes Operator model for the building, deployment, and management of its runtime. Once deployed, the runtime can scale as any other stateless cloud-native workload on Kubernetes and integrate with other cloud-native projects. For example, it can use  for single sign-on and data roles,  for distributed caching needs, export metrics and register with Prometheus for monitoring, Jaeger for tracing, and even with for API management. But ultimately, Teiid runs as a single Spring Boot application acting as a data proxy and integrating with other best-of-breed services on Openshift rather than trying to reinvent everything from scratch. Architectural overview of Teiid data gateway On the client-side, Teiid offers standard SQL over JDBC/ODBC and Odata APIs. Business users, analysts, and data scientists can use standard BI/analytics tools such as Tableau, MicroStrategy, Spotfire, etc. to interact with Teiid. Developers can leverage the REST API or JDBC for custom built microservices and serverless workloads. In either case, for data consumers, Teiid appears as a standard PostgreSQL database accessed over its JDBC or ODBC protocols but offering additional abstractions and decoupling from the physical data sources.  is another popular open-source project started by Facebook. It is a distributed SQL query engine targeting big data use cases through its coordinator-worker architecture. The Coordinator is responsible for parsing statements, planning queries, managing workers, fetching results from the workers, and returning the final results to the client. The worker is responsible for executing tasks and processing data.  Some time ago, the founders split from PrestoDB and created a fork called (formerly PrestoSQL). Today, PrestoDB is part of The Linux Foundation, and Trino part of Trino Software Foundation. Both distributions of Presto are among the most active and powerful open-source data gateway projects in this space. To learn more about this technology, is a good book I found. CLOUD-HOSTED DATA GATEWAYS SERVICES With a move to the cloud infrastructure, the need for data gateways doesn’t go away but increases instead. Here are a few cloud-based data gateway services:  is ANSI SQL based interactive query service for analyzing data tightly integrated with Amazon S3. It is based on PrestoDB and supports additional data sources and federation capabilities too. Another similar service by Amazon is . It is focused around the same functionality, i.e. querying S3 objects using SQL. The main difference is that Redshift Spectrum requires a Redshift cluster, whereas Athena is a serverless offering that doesn’t require any servers.  is a similar service but from Google. These tools require minimal to no setup, they can access on-premise or cloud-hosted data and process huge datasets. But they couple you with a single cloud provider as they cannot be deployed on multiple clouds or on-premise. They are ideal for interactive querying rather than acting as hybrid data frontend for other services and tools to use. SECURE TUNNELING DATA-PROXIES With cloud-hosted data gateways comes the need for accessing on-premise data. Data has gravity and also might be affected by regulatory requirements preventing it from moving to the cloud. It may also be a conscious decision to keep the most valuable asset (your data) from cloud-coupling. All of these cases require cloud access to on-premise data. And cloud providers make it easy to reach your data. Azure’s  is such a proxy allowing access to on-premise data stores from Azure Service Bus. In the opposite scenario, accessing cloud-hosted data stores from on-premise clients can be challenging too. Google’s  provides secure access to Cloud SQL instances without having to whitelist IP addresses or configure SSL. Red Hat-sponsored open-source project  takes the more generic approach to address these challenges. Skupper solves Kubernetes multi-cluster communication challenges through a layer 7 virtual network that offers advanced routing and secure connectivity capabilities. Rather than embedding Skupper into the business service runtime, it runs as a standalone instance per Kubernetes namespace and acts as a shared sidecar capable of secure tunneling for data access or other general service-to-service communication. It is a generic secure-connectivity proxy applicable for many use cases in the hybrid cloud world. CONNECTION POOLS FOR SERVERLESS WORKLOADS Serverless takes software decomposition a step further from microservices. Rather than services splitting by bounded context, serverless is based on the function model where every operation is short-lived and performs a single operation. These granular software constructs are extremely scalable and flexible but come at a cost that previously wasn’t present. It turns out rapid scaling of functions is a challenge for connection-oriented data sources such as relational databases and message brokers. As a result cloud providers offer transparent data proxies as a service to manage connection pools effectively.  is such a service that sits between your application and your relational database to efficiently manage connections to the database and improve scalability. CONCLUSION Modern cloud-native architectures combined with the microservices principles enable the creation of highly scalable and independent applications. The large choice of data storage engines, cloud-hosted services, protocols, and data formats, gives the ultimate flexibility for delivering software at a fast pace. But all of that comes at a cost that becomes increasingly visible with the need for uniform real-time data access from emerging user groups with different needs. Keeping microservices data only for the microservice itself creates challenges that have no good technological and architectural answers yet. Data gateways, combined with cloud-native technologies offer features similar to API gateways but for the data layer that can help address these new challenges. The data gateways vary in specialization, but they tend to consolidate on providing uniform SQL-based access, enhanced security with data roles, caching, and abstraction over physical data stores. Data has gravity, requires granular access control, is hard to scale, and difficult to move on/off/between cloud-native infrastructures. Having a data gateway component as part of the cloud-native tooling arsenal, which is hybrid and works on multiple cloud providers, supports different use cases is becoming a necessity. This article was originally published on InfoQ .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/r9Na-7W7j-Y" height="1" width="1" alt=""/&gt;</content><dc:creator>Unknown</dc:creator><feedburner:origLink>http://www.ofbizian.com/2021/05/data-gateways-of-future.html</feedburner:origLink></entry></feed>
